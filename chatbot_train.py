# -*- coding: utf-8 -*-
"""mentalhealthbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zHGOOy38hLBKBqGEnZKjKBclbe0rnmRV
"""
#import the required libraries
import numpy as np
import pandas as pd
import nltk
import string
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input,Embedding,LSTM,Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import json

# Load the intent dataset from JSON file
with open('intent.json') as content:
  data1=json.load(content)

# Initialize lists to store input text, tags, and responses
tags=[]
inputs=[]
responses={}

# Extract intents, responses, and input text
for intent in data1['intents']:
  responses[intent['tag']]=intent['responses']
  for lines in intent['input']:
    inputs.append(lines)
    tags.append(intent['tag'])

# Create a DataFrame with inputs and corresponding tags
data=pd.DataFrame({"inputs":inputs,"tags":tags})
data

data=data.sample(frac=1)# Shuffle the dataset to improve training randomness
# Data Preprocessing: Convert to lowercase and remove punctuation
data['inputs']=data['inputs'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])
data['inputs']=data['inputs'].apply(lambda wrd:''.join(wrd))
data

# Initialize a tokenizer and fit it on input tex
tokenizer=Tokenizer(num_words=2000)
tokenizer.fit_on_texts(data['inputs'])
train=tokenizer.texts_to_sequences(data['inputs'])# Convert text into numerical sequences

x_train=pad_sequences(train)# Pad sequences to ensure uniform input size

# Encode target labels (tags) as numerical values
le=LabelEncoder()
y_train=le.fit_transform(data['tags'])

input_shape=x_train.shape[1]
print(input_shape)

vocabulary=len(tokenizer.word_index)
print("number of unique words:",vocabulary)
output_length=le.classes_.shape[0]
print("output length:",output_length)

# Define the neural network model
i=Input(shape=(input_shape,))
x=Embedding(vocabulary+1,10)(i)# Word embeddings
x=LSTM(10,return_sequences=True)(x)
x=Flatten()(x)
x=Dense(output_length,activation="softmax")(x)
model=Model(i,x)

# Compile the model with categorical cross-entropy loss and Adam optimizer
model.compile(loss="sparse_categorical_crossentropy",optimizer="adam",metrics=["accuracy"])

# Train the model
train=model.fit(x_train,y_train,epochs=200)

# Plot training accuracy and loss
plt.plot(train.history['accuracy'],label='training set accuracy')
plt.plot(train.history['loss'],label='training set loss')
plt.legend()

import pickle

# Save the trained Keras model
model.save('chatbot_model.keras')

# Save the tokenizer
with open('tokenizer.pkl', 'wb') as tokenizer_file:
    pickle.dump(tokenizer, tokenizer_file)

# Save the label encoder
with open('label_encoder.pkl', 'wb') as le_file:
    pickle.dump(le, le_file)
